# -*- coding: utf-8 -*-
"""EnsembleTech_Project_ShriyaNatesan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iHeJlKyZNNBGt_cDXMDdieDXTCgIxWXc
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

!pip install mysql-connector-python

"""## DATA IMPORTING

### In my local machine I have installed, MySQL Server and workbench with essential connectors for python.

### Now, I will create a DB through python by connecting to the MySQL Server.
"""

import mysql.connector

mydb = mysql.connector.connect(
  host="127.0.0.1",
  user="root",
  passwd="root",
  port="3306"
)

print(mydb)

"""### We have successfully connected to the MySQL Server running on 127.0.0.1:3306"""

mycursor = mydb.cursor()

mycursor.execute("CREATE DATABASE mydb")
## if databse doesn't exist

mydb = mysql.connector.connect(
  host="127.0.0.1",
  user="root",
  passwd="root",
  port="3306",
  database="mydb"
)

"""### Have created a DB and succesfully connected to it

### Using MySQL Workbench I have created two tables: telcomcustomer-churn_1 and telcomcustomer-churn_2

### telcomcustomer-churn_1 has part1 of dataset and telcomcustomer-churn_2 has part2 of dataset

### The datasets have been loaded to the table using the import csv option

#### telcomcustomer-churn_1 table snapshot ->

![tab1.PNG](attachment:tab1.PNG)

#### telcomcustomer-churn_2 table snapshot ->

![tab2.PNG](attachment:tab2.PNG)

### Importing table 1
"""

mycursor = mydb.cursor()
sql = "SELECT * FROM `telcomcustomer-churn_1`"
mycursor.execute(sql)

myresult = mycursor.fetchall()

for x in myresult:
    print(x)

from pandas import DataFrame
df = DataFrame(myresult)

sequence = mycursor.column_names
sequence

df.columns = sequence

df

"""### Importing table 2"""

mycursor = mydb.cursor()
sql = "SELECT * FROM `telcomcustomer-churn_2`"
mycursor.execute(sql)

myresult = mycursor.fetchall()

for x in myresult:
    print(x)

from pandas import DataFrame
df2 = DataFrame(myresult)

sequence2 = mycursor.column_names
sequence2

df2.columns = sequence2
df2

print("Dimension of Table1: {}".format(df.shape))
#The Data1 consists of 7043 data points, with 10 features

df.dtypes

print("Dimension of Table2: {}".format(df2.shape))
#The Data1 consists of 7032 data points, with 11 features

df2.dtypes

"""## MERGING DATASETS"""

## this performs a join and thus for table 1 we have all the rows, table 2 has lesser rows so that would be filled with NA
cust2 = pd.concat([df, df2], axis=1)
cust2

"""## DATA CLEANSING"""

# The first column is customerid column which has nothing to do with the churn value. So drop it.
cust2 = cust2.drop(labels = "customerID", axis = 1)

cust2.groupby('Churn').size()

cust2.isnull().sum()

cust2.dtypes

## Since there are only 11 NA columns so that would not cause big difference if they are removed. Hence dropping them

data2 = cust2
data2 = data2.dropna()

# thus our final dataset with 7032 rows and 20 columns
data2.shape

data2.info()

for feature in data2.columns: # Loop through all columns in the dataframe
    if data2[feature].dtype == 'object': # Only apply for columns with categorical strings
        data2[feature] = pd.Categorical(data2[feature])# Replace strings with an integer
data2.head(10)

print(data2.gender.value_counts())
print(data2.Partner.value_counts())
print(data2.Dependents.value_counts())
print(data2.PhoneService.value_counts())
print(data2.MultipleLines.value_counts())
print(data2.InternetService.value_counts())
print(data2.OnlineSecurity.value_counts())
print(data2.OnlineBackup.value_counts())
print(data2.DeviceProtection.value_counts())
print(data2.TechSupport.value_counts())
print(data2.StreamingTV.value_counts())
print(data2.StreamingMovies.value_counts())
print(data2.Contract.value_counts())
print(data2.PaperlessBilling.value_counts())
print(data2.PaymentMethod.value_counts())
print(data2.Churn.value_counts())

replaceStruct = {
                "gender"     :    {"Female": 1,"Male": 0},
                "Partner"    :    {"Yes": 1, "No": 0},
                "Dependents" :    {"Yes": 1, "No": 0},
                "PhoneService":   {"Yes": 1, "No": 0},
                "MultipleLines":  {"Yes": 1, "No": 0, "No phone service": -1 },
                "InternetService": {"Fiber optic" : 2, "DSL" : 1, "No" : 0},
                "OnlineSecurity" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "OnlineBackup" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "DeviceProtection" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "TechSupport" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "StreamingTV" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "StreamingMovies" : {"Yes": 1, "No": 0, "No internet service": -1 },
                "Contract" : {"Month-to-month": 0, "One year": 1, "Two year": 2 },
                "PaperlessBilling"    :    {"Yes": 1, "No": 0},
                "PaymentMethod" : {"Electronic check" : 1, "Mailed check" : 2, "Bank transfer (automatic)" : 3, "Credit card (automatic)" : 4 },
                "Churn"    :    {"Yes": 1, "No": 0},
                }

data2=data2.replace(replaceStruct)
data2.head(10)

"""## DATA ANALYSIS & VISUALISATION"""

data2.describe().transpose()

# Commented out IPython magic to ensure Python compatibility.
#plot the graphs of different variable to see the distributions.

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

sns.countplot(data2['Churn'])

sns.countplot(data2['PaymentMethod'],hue=data2['Churn'])

##"PaymentMethod":{"Electronic check" : 1, "Mailed check" : 2, "Bank transfer (automatic)" : 3, "Credit card (automatic)" : 4 },
##"Churn":{"Yes": 1, "No": 0},

## Highest electronic check customer have churned =  yes

colormap = plt.cm.viridis # Color range to be used in heatmap
plt.figure(figsize=(15,15))
plt.title('Pearson Correlation of attributes', y=1.05, size=19)
sns.heatmap(data2.corr(),linewidths=0.1,vmax=1.0,
            square=True, cmap=colormap, linecolor='white', annot=True)

#We see churn is highly negatively correlated to Contract
#We see Streaming TV and Streaming Movies are highly positively correlated to Monthly charges
#We see StreamingTV is highly correalted to StreamingMovies attribute (means one who streams movies streams Tv also)

feature_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']
import seaborn as sns
sns.pairplot(data2[feature_columns], diag_kind = 'kde')
#We see Monthly charges and TotalCharges are negatively corrleated

sns.pairplot(data2[['StreamingTV','StreamingMovies']])
#shows their high correlation

numeric_col = data2.columns[pd.Series(data2.columns).apply(lambda x : data2[x].dtype == 'float64')]  # Non object columns

plt.figure(figsize = (20,3))

j = 1
for i in numeric_col:
    plt.subplot(1,2,j)
    sns.boxplot(data2[i])
    j += 1

"""Monthly charges seems to be left skewed, and Total Charges seems to be right skewed. Though both have almost no outliers in the data."""

plt.figure(figsize=(20,10))
#Customer's age and the HighestSpend relation
sns.barplot(data2['tenure'], data2['TotalCharges'])

plt.figure(figsize=(10,5))
#Customer's age and the Internet Banking relation
sns.countplot(data2['SeniorCitizen'],hue=data2['PaperlessBilling'])

"""## DATA PRE-PROCESSING"""

X = data2.drop("Churn", axis=1)
y = data2['Churn']

#Checking for class imbalance
y.value_counts()
#shows totally imbalanced target variable here

# Class count
count_class_0, count_class_1 = y.value_counts()

# Divide by class
df_class_0 = data2[data2['Churn'] == 0]
df_class_1 = data2[data2['Churn'] == 1]

"""### Random undersampling"""

df_class_0_under = df_class_0.sample(count_class_1)
df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)

print('Random under-sampling:')
print(df_test_under.Churn.value_counts())

df_test_under.Churn.value_counts().plot(kind='bar', title='Count (target)');

colormap = plt.cm.viridis # Color range to be used in heatmap
plt.figure(figsize=(15,15))
plt.title('Pearson Correlation of attributes', y=1.05, size=19)
sns.heatmap(df_test_under.corr(),linewidths=0.1,vmax=1.0,
            square=True, cmap=colormap, linecolor='white', annot=True)

"""### Random oversampling"""

df_class_1_over = df_class_1.sample(count_class_0, replace=True)
df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)

print('Random over-sampling:')
print(df_test_over.Churn.value_counts())

df_test_over.Churn.value_counts().plot(kind='bar', title='Count (target)');

colormap = plt.cm.viridis # Color range to be used in heatmap
plt.figure(figsize=(15,15))
plt.title('Pearson Correlation of attributes', y=1.05, size=19)
sns.heatmap(df_test_over.corr(),linewidths=0.1,vmax=1.0,
            square=True, cmap=colormap, linecolor='white', annot=True)

"""## MODEL TRAINING, TESTING AND TUNING

### Using the undersampled data
"""

X_under = df_test_under.drop("Churn", axis=1)
y_under = df_test_under['Churn']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=.30, random_state=1)

X_under.describe().transpose()

X_train.describe().transpose()
## Yes its almost same to the original data

X_test.describe().transpose()
## Yes its almost same to the original data

from sklearn.tree import DecisionTreeClassifier

dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)

print(dTree.score(X_train, y_train))
print(dTree.score(X_test, y_test))

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

fn = list(X_train)
cn = ['No', 'Yes']
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4, 4), dpi=300)
plot_tree(dTree, feature_names = fn, class_names=cn, filled = True)

fig.savefig('tree.png')

"""1. BAGGING"""

from sklearn.ensemble import BaggingClassifier

bgcl = BaggingClassifier(base_estimator=dTree, n_estimators=50,random_state=1)
#bgcl = BaggingClassifier(n_estimators=50,random_state=1)

bgcl = bgcl.fit(X_train, y_train)

y_predict = bgcl.predict(X_test)

print(bgcl.score(X_test , y_test))

"""2. BOOSTING"""

from sklearn.ensemble import AdaBoostClassifier
abcl = AdaBoostClassifier(n_estimators=30, random_state=1)
#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)
abcl = abcl.fit(X_train, y_train)

y_predict = abcl.predict(X_test)
print(abcl.score(X_test , y_test))

from sklearn.ensemble import GradientBoostingClassifier
gbcl = GradientBoostingClassifier(n_estimators = 40,random_state=1)
gbcl = gbcl.fit(X_train, y_train)

y_predict = gbcl.predict(X_test)
print(gbcl.score(X_test, y_test))

from sklearn.ensemble import RandomForestClassifier
rfcl = RandomForestClassifier(n_estimators = 40, random_state=1,max_features=12)
rfcl = rfcl.fit(X_train, y_train)

y_predict = rfcl.predict(X_test)
print(rfcl.score(X_test, y_test))

"""3. CHANGING HYPERPARAMETERS"""

dTreeE = DecisionTreeClassifier(criterion = 'entropy', random_state=1)
dTreeE.fit(X_train, y_train)
print(dTreeE.score(X_train, y_train))
print(dTreeE.score(X_test, y_test))

dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, random_state=1)
dTreeR.fit(X_train, y_train)
print(dTreeR.score(X_train, y_train))
print(dTreeR.score(X_test, y_test))

dTreeM = DecisionTreeClassifier(criterion = 'gini', min_samples_split = 20, random_state=1)
dTreeM.fit(X_train, y_train)
print(dTreeM.score(X_train, y_train))
print(dTreeM.score(X_test, y_test))

dTreeI = DecisionTreeClassifier(criterion = 'entropy', min_impurity_decrease = 1.0, random_state=1)
dTreeI.fit(X_train, y_train)
print(dTreeI.score(X_train, y_train))
print(dTreeI.score(X_test, y_test))

def fit_n_print(model, X_train, X_test, y_train, y_test):  # take the model, train data and test data as input

    start = time.time()  # note the start time

    model.fit(X_train, y_train)   # fit the model using the train data

    pred = model.predict(X_test)     # model predictions on the test data

    r2 = metrics.r2_score(y_test, pred)  # calculate the r squared value on the test data

    rmse = sqrt(metrics.mean_squared_error(y_test, pred))   # Root mean squared error

    rmsle = sqrt(metrics.mean_squared_log_error(y_test, pred))  # Root mean squared log error (since the numbers are huge)

    scorer = {'r2' : metrics.make_scorer(metrics.r2_score),
              'mse' : metrics.make_scorer(metrics.mean_squared_error),
              'msle' : metrics.make_scorer(metrics.mean_squared_log_error),
               }    # make scorers to be used in cross validation

    cv = cross_validate(model, X_train, y_train, cv=10, scoring = scorer)   # perform cross validation accross 3 metrics

    r2_cv = cv['test_r2'].mean()                                     # mean r squared value
    rmse_cv = np.mean([sqrt(mse) for mse in cv['test_mse']]) # mean RMSE value(take root of individual mse value and then mean)
    rmsle_cv = np.mean([sqrt(msle) for msle in cv['test_msle']]) # mean RMSLE

    end = time.time()  #note the end time


    duration = end - start  # calculate the total duration


    return r2, rmse, rmsle, r2_cv, rmse_cv, rmsle_cv, duration, pred  # return all the metrics along with predictions

result = {}   # Create an empty dictionary to later use to store metrics of each of the models

# putting all 5 models in a for loop and appending the results of each of the models to the 'result' dictionary
for model, name  in zip([dTree, dTreeE, bgcl, abcl, gbcl, rfcl], ['Decision Tree - GINI', 'Decision Tree - Entropy',
                                                                  'Bagging Ensemble', 'Ada Boost Ensemble',
                                                                  'Gradient Boost Ensemble','Random Forest']):
    result[name] = fit_n_print(model,X_train, X_test,y_train, y_test)
    # store all the metrics in the result dict, with name as key

result1 = pd.DataFrame(np.array(list(result.values()))[:,:-1],    # make a dataframe out of the metrics from result dictionary
                       columns= ['R Squared', 'MSE', 'MSLE', 'R2 CV', 'RMSE CV', 'RMSLE CV', 'Elapsed'],
                      index= result.keys())   # use the model names as index

result1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'

result1

"""### Observations
1. Here, for the undersampled data thus through accuracy score and R squared error score, Ada boost classifier works well.
2. Ada boost takes the 4th best amount of time to build the tree and predict the values
3. Here R squared values for the algos dont turn out to be very negative values.

### Using the oversampled data
"""

X_over = df_test_over.drop("Churn", axis=1)
y_over = df_test_over['Churn']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=.30, random_state=1)

X_over.describe().transpose()

X_train.describe().transpose()
## Yes its almost same to the original data

X_test.describe().transpose()
## Yes its almost same to the original data

from sklearn.tree import DecisionTreeClassifier

dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)
dTree.fit(X_train, y_train)

print(dTree.score(X_train, y_train))
print(dTree.score(X_test, y_test))

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

fn = list(X_train)
cn = ['No', 'Yes']
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4, 4), dpi=300)
plot_tree(dTree, feature_names = fn, class_names=cn, filled = True)

fig.savefig('tree2.png')

"""1. BAGGING"""

from sklearn.ensemble import BaggingClassifier

bgcl = BaggingClassifier(base_estimator=dTree, n_estimators=50,random_state=1)
#bgcl = BaggingClassifier(n_estimators=50,random_state=1)

bgcl = bgcl.fit(X_train, y_train)

y_predict = bgcl.predict(X_test)

print(bgcl.score(X_train , y_train))
print(bgcl.score(X_test , y_test))

"""2. BOOSTING"""

from sklearn.ensemble import AdaBoostClassifier
abcl = AdaBoostClassifier(n_estimators=30, random_state=1)
#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)
abcl = abcl.fit(X_train, y_train)

y_predict = abcl.predict(X_test)
print(abcl.score(X_test , y_test))

from sklearn.ensemble import GradientBoostingClassifier
gbcl = GradientBoostingClassifier(n_estimators = 40,random_state=1)
gbcl = gbcl.fit(X_train, y_train)

y_predict = gbcl.predict(X_test)
print(gbcl.score(X_test , y_test))

from sklearn.ensemble import RandomForestClassifier
rfcl = RandomForestClassifier(n_estimators = 40, random_state=1,max_features=12)
rfcl = rfcl.fit(X_train, y_train)

y_predict = rfcl.predict(X_test)
print(rfcl.score(X_test , y_test))

"""3. CHANGING HYPERPARAMETERS"""

dTreeR = DecisionTreeClassifier(criterion = 'entropy', random_state=1, max_features=8)
dTreeR.fit(X_train, y_train)
print(dTreeR.score(X_train, y_train))
print(dTreeR.score(X_test, y_test))

dTreeMD = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, random_state=1)
dTreeMD.fit(X_train, y_train)
print(dTreeMD.score(X_train, y_train))
print(dTreeMD.score(X_test, y_test))

dTreeM = DecisionTreeClassifier(criterion = 'gini', min_samples_split = 50, random_state=1)
dTreeM.fit(X_train, y_train)
print(dTreeM.score(X_train, y_train))
print(dTreeM.score(X_test, y_test))

dTreeI = DecisionTreeClassifier(criterion = 'entropy', min_impurity_decrease = 1.0, random_state=1)
dTreeI.fit(X_train, y_train)
print(dTreeI.score(X_train, y_train))
print(dTreeI.score(X_test, y_test))

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

xgb = XGBClassifier()
cgb = CatBoostClassifier()
lgb = LGBMClassifier()

result = {}   # Create an empty dictionary to later use to store metrics of each of the models

# putting all 5 models in a for loop and appending the results of each of the models to the 'result' dictionary
for model, name  in zip([dTree, dTreeR, bgcl, abcl, gbcl, rfcl,xgb, cgb, lgb], ['Decision Tree - GINI', 'Decision Tree - Entropy',
                                                                  'Bagging Ensemble', 'Ada Boost Ensemble',
                                                                  'Gradient Boost Ensemble','Random Forest',
                                                                 'Xgboost', 'Catboost','LightGB']):
    result[name] = fit_n_print(model,X_train, X_test,y_train, y_test)
    # store all the metrics in the result dict, with name as key

result1 = pd.DataFrame(np.array(list(result.values()))[:,:-1],    # make a dataframe out of the metrics from result dictionary
                       columns= ['R Squared', 'MSE', 'MSLE', 'R2 CV', 'RMSE CV', 'RMSLE CV', 'Elapsed'],
                      index= result.keys())   # use the model names as index

result1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'

result1

"""### Observations
1. Here after comparing the accuracy scores and the R squared error values, **Bagging Ensemble** seems to be the most fit.
2. Ada Boost, Random Forest & Xgboost comes quite close to Bagging ensemble while comparison
3. BaggingEnsemble gives good results but takes almost a good amount of time to estimate and predict the values as compared to other algos leaving the Xgboost, Catboost and LightGb boost algos
4. Here R squared values for the algos dont turn out to be very negative as in undersampled data values.
5. LightGB and GB ensemble took quite less time for constructing the model as compared to otherboosting alsos

### Selecting best fit model
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

print(bgcl.score(X_test , y_test))
y_predict = bgcl.predict(X_test)

cm = confusion_matrix(y_test, y_predict, labels=[0, 1])

df_cm = pd.DataFrame(cm, index = [i for i in ["No","Yes"]],
                  columns = [i for i in ["No","Yes"]])
plt.figure(figsize = (7,5))
sns.heatmap(df_cm, annot=True ,fmt='g')

"""### Finalizing the model

1. Here, we see that the TP and TN no.s are quite good.
2. The people who might churn and are predicted that thwy wont churn is only 74, which is quite less as compared to the true positves and negatives and can be handled by the company.
3. Here the motive is that the ones who might not churn but are predicted to churn is 279 which gives the company a better window buffer.
4. Here since its a oversampled data, the bagging classifer works well because the number of yes value samples were quite less and it was oversampled thus the bagging classifier is used in cases where the dataset has less values and the sample size is quite large which is same as our case.

Hence we go with the **Bagging Ensemble built on the Oversampled data model** for using in the GUI prediction

## GUI TO PREDICT CUSTOMER CHURN
"""

from tkinter import *

def click():
    e1 = tkvar1.get() #gender
    e2 = tkvar2.get() #SC
    e3 = tkvar3.get() #Partner
    e4 = tkvar4.get() #dependents
    e5 = text1.get() #tenure
    e6 = tkvar5.get()
    e7 = tkvar6.get()
    e8 = tkvar7.get()
    e9 = tkvar8.get()
    e10 = tkvar9.get()
    e11 = tkvar10.get()
    e12 = tkvar11.get()
    e13 = tkvar12.get()
    e14 = tkvar13.get()
    e15 = tkvar14.get()
    e16 = tkvar15.get()
    e17 = tkvar16.get()
    e18 = text2.get()
    e19 = text3.get()

    dataTest = X_test.iloc[0,:]

    #gender
    if(e1 == "Female"):
        dataTest.iloc[0] = 1.0
    elif(e1 == "Male"):
        dataTest.iloc[0] = 0.0

    #SC
    if(e2 == 1):
        dataTest.iloc[1] = 1.0
    elif(e2 == 0):
        dataTest.iloc[1] = 0.0

    #Partner
    if(e3 == "Yes"):
        dataTest.iloc[2] = 1.0
    elif(e3 == "No"):
        dataTest.iloc[2] = 0.0

    #dependents
    if(e4 == "Yes"):
        dataTest.iloc[3] = 1.0
    elif(e4 == "No"):
        dataTest.iloc[3] = 0.0

    #tenure
    dataTest.iloc[4] = e5

    #PhoneService
    if(e6 == "Yes"):
        dataTest.iloc[5] = 1.0
    elif(e6 == "No"):
        dataTest.iloc[5] = 0.0

    #Multiplelines
    if(e7 == "Yes"):
        dataTest.iloc[6] = 1.0
    elif(e7 == "No"):
        dataTest.iloc[6] = 0.0
    elif(e7 == "No phone service"):
        dataTest.iloc[6] = -1.0

    #internet Service
    if(e8 == "Fiber optic"):
        dataTest.iloc[7] = 2.0
    elif(e8 == "DSL"):
        dataTest.iloc[7] = 1.0
    elif(e8 == "No"):
        dataTest.iloc[7] = 0.0

    #online sec
    if(e9 == "Yes"):
        dataTest.iloc[8] = 1.0
    elif(e9 == "No"):
        dataTest.iloc[8] = 0.0
    elif(e9 == "No internet service"):
        dataTest.iloc[8] = -1.0

    #online backup
    if(e10 == "Yes"):
        dataTest.iloc[9] = 1.0
    elif(e10 == "No"):
        dataTest.iloc[9] = 0.0
    elif(e10 == "No internet service"):
        dataTest.iloc[9] = -1.0

    #device prot
    if(e11 == "Yes"):
        dataTest.iloc[10] = 1.0
    elif(e11 == "No"):
        dataTest.iloc[10] = 0.0
    elif(e11 == "No internet service"):
        dataTest.iloc[10] = -1.0

    #tech sup
    if(e12 == "Yes"):
        dataTest.iloc[11] = 1.0
    elif(e12 == "No"):
        dataTest.iloc[11] = 0.0
    elif(e12 == "No internet service"):
        dataTest.iloc[11] = -1.0

    #S T
    if(e13 == "Yes"):
        dataTest.iloc[12] = 1.0
    elif(e13 == "No"):
        dataTest.iloc[12] = 0.0
    elif(e13 == "No internet service"):
        dataTest.iloc[12] = -1.0

    #S M
    if(e14 == "Yes"):
        dataTest.iloc[13] = 1.0
    elif(e14 == "No"):
        dataTest.iloc[13] = 0.0
    elif(e14 == "No internet service"):
        dataTest.iloc[13] = -1.0

    #contract
    if(e15 == "Month-to-month"):
        dataTest.iloc[14] = 0.0
    elif(e15 == "One year"):
        dataTest.iloc[14] = 1.0
    elif(e15 == "Two year"):
        dataTest.iloc[14] = 2.0

    #paperless bill
    if(e16 == "Yes"):
        dataTest.iloc[15] = 1.0
    elif(e16 == "No"):
        dataTest.iloc[15] = 1.0

    #Payment method
    if(e15 == "Electronic check"):
        dataTest.iloc[14] = 1.0
    elif(e15 == "Mailed check"):
        dataTest.iloc[14] = 2.0
    elif(e15 == "Bank transfer (automatic)"):
        dataTest.iloc[14] = 3.0
    elif(e15 == "Credit card (automatic)"):
        dataTest.iloc[14] = 4.0

    #M C
    dataTest.iloc[17] = e18
    #T C
    dataTest.iloc[18] = e19

    print(dataTest)

    pred_output = bgcl.predict(dataTest.values.reshape(1, -1))
    print(pred_output[0])

    if(pred_output[0] == 1.0):
        outputText = "Yes"
    elif(pred_output[0] == 0.0):
        outputText = "No"

    output.insert(END, outputText)



window = Tk()
window.title("My Customer Churn Prediction System")
window.geometry("800x1200")
window['background']='#0ec7c7'


Label(window, text="Enter the following data to predict customer churn",bg = "#0ec7c7",fg="black",font="none 12 bold").grid(row=0, column=0, sticky=W)

# Create a Tkinter variable
tkvar1 = StringVar(window)

# Dictionary with options
Genderchoices = { 'Male','Female'}

popupMenu1 = OptionMenu(window, tkvar1, *Genderchoices)
Label(window, text="Gender", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 2, column = 0, sticky=W)
popupMenu1.config(width= 20, bg="white")
popupMenu1.grid(row = 2, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown(*args):
    gender = tkvar1.get()
    print(gender)

# link function to change dropdown
tkvar1.trace('w', change_dropdown)

################################################

# Create a Tkinter variable
tkvar2 = IntVar(window)

# Dictionary with options
intchoices = { 0, 1}
tkvar2.set(0)

popupMenu2 = OptionMenu(window, tkvar2, *intchoices)
Label(window, text="Senior Citizen", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 3, column = 0, sticky=W)
popupMenu2.config(width= 20, bg="white")
popupMenu2.grid(row = 3, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown2(*args):
    SC = tkvar2.get()
    print(SC)

# link function to change dropdown
tkvar2.trace('w', change_dropdown2)

################################################

# Create a Tkinter variable
tkvar3 = StringVar(window)

# Dictionary with options
YNchoices = { "Yes", "No"}

popupMenu3 = OptionMenu(window, tkvar3, *YNchoices)
Label(window, text="Partner", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 4, column = 0, sticky=W)
popupMenu3.config(width= 20, bg="white")
popupMenu3.grid(row = 4, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown3(*args):
    Partner = tkvar3.get()
    print(Partner)

# link function to change dropdown
tkvar3.trace('w', change_dropdown3)

################################################

# Create a Tkinter variable
tkvar4 = StringVar(window)

popupMenu4 = OptionMenu(window, tkvar4, *YNchoices)
Label(window, text="Dependents", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 5, column = 0, sticky=W)
popupMenu4.config(width= 20, bg="white")
popupMenu4.grid(row = 5, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown4(*args):
    Dependents = tkvar4.get()


# link function to change dropdown
tkvar4.trace('w', change_dropdown4)

################################################

Label(window, text="Tenure",bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 6, column=0, sticky=W)
text1 = Entry(window, width= 27, bg="white")
text1.grid(row=6, column=1, pady=3, sticky=W)

################################################

# Create a Tkinter variable
tkvar5 = StringVar(window)

popupMenu5 = OptionMenu(window, tkvar5, *YNchoices)
Label(window, text="Phone Service", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 7, column = 0, sticky=W)
popupMenu5.config(width= 20, bg="white")
popupMenu5.grid(row = 7, column =1, pady=3, sticky=W )

# on change dropdown value
def change_dropdown5(*args):
    PhoneS = tkvar5.get()
    print(PhoneS)

# link function to change dropdown
tkvar5.trace('w', change_dropdown5)

################################################

# Dictionary with options
Phonechoices = {"Yes","No", "No phone service"}

# Create a Tkinter variable
tkvar6 = StringVar(window)

popupMenu6 = OptionMenu(window, tkvar6, *Phonechoices)
Label(window, text="Multiple Lines", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 8, column = 0, sticky=W)
popupMenu6.config(width= 20, bg="white")
popupMenu6.grid(row = 8, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown6(*args):
    ML = tkvar6.get()
    print(ML)

# link function to change dropdown
tkvar6.trace('w', change_dropdown6)

################################################

# Dictionary with options
Internetchoices = {"Fiber optic", "DSL", "No"}

# Create a Tkinter variable
tkvar7 = StringVar(window)

popupMenu7 = OptionMenu(window, tkvar7, *Internetchoices)
Label(window, text="Internet Service", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 9, column = 0, sticky=W)
popupMenu7.config(width= 20, bg="white")
popupMenu7.grid(row = 9, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown7(*args):
    IN = tkvar7.get()
    print(IN)

# link function to change dropdown
tkvar7.trace('w', change_dropdown7)

################################################

# Dictionary with options
Ichoices = {"Yes","No", "No internet service"}

# Create a Tkinter variable
tkvar8 = StringVar(window)

popupMenu8 = OptionMenu(window, tkvar8, *Ichoices)
Label(window, text="Online Security", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 10, column = 0, sticky=W)
popupMenu8.config(width= 20, bg="white")
popupMenu8.grid(row = 10, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown8(*args):
    OS = tkvar8.get()
    print(OS)

# link function to change dropdown
tkvar8.trace('w', change_dropdown8)

################################################

# Create a Tkinter variable
tkvar9 = StringVar(window)

popupMenu9 = OptionMenu(window, tkvar9, *Ichoices)
Label(window, text="Online Backup", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 11, column = 0, sticky=W)
popupMenu9.config(width= 20, bg="white")
popupMenu9.grid(row = 11, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown9(*args):
    OB = tkvar9.get()
    print(OB)

# link function to change dropdown
tkvar9.trace('w', change_dropdown9)

################################################

# Create a Tkinter variable
tkvar10 = StringVar(window)

popupMenu10 = OptionMenu(window, tkvar10, *Ichoices)
Label(window, text="Device Protection", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 12, column = 0, sticky=W)
popupMenu10.config(width= 20, bg="white")
popupMenu10.grid(row = 12, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown10(*args):
    DP = tkvar10.get()
    print(DP)

# link function to change dropdown
tkvar10.trace('w', change_dropdown10)

################################################

# Create a Tkinter variable
tkvar11 = StringVar(window)

popupMenu11 = OptionMenu(window, tkvar11, *Ichoices)
Label(window, text="Tech Support", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 13, column = 0, sticky=W)
popupMenu11.config(width= 20, bg="white")
popupMenu11.grid(row = 13, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown11(*args):
    TS = tkvar11.get()
    print(TS)

# link function to change dropdown
tkvar11.trace('w', change_dropdown11)

################################################

# Create a Tkinter variable
tkvar12 = StringVar(window)

popupMenu12 = OptionMenu(window, tkvar12, *Ichoices)
Label(window, text="Streaming TV", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 14, column = 0, sticky=W)
popupMenu12.config(width= 20, bg="white")
popupMenu12.grid(row = 14, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown12(*args):
    ST = tkvar12.get()
    print(ST)

# link function to change dropdown
tkvar12.trace('w', change_dropdown12)

################################################

# Create a Tkinter variable
tkvar13 = StringVar(window)

popupMenu13 = OptionMenu(window, tkvar13, *Ichoices)
Label(window, text="Streaming Movies", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 15, column = 0, sticky=W)
popupMenu13.config(width= 20, bg="white")
popupMenu13.grid(row = 15, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown13(*args):
    SM = tkvar13.get()
    print(SM)

# link function to change dropdown
tkvar13.trace('w', change_dropdown13)

################################################


# Dictionary with options
Cchoices = {"Month-to-month", "One year", "Two year"}

# Create a Tkinter variable
tkvar14 = StringVar(window)

popupMenu14 = OptionMenu(window, tkvar14, *Cchoices)
Label(window, text="Contract", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 16, column = 0, sticky=W)
popupMenu14.config(width= 20, bg="white")
popupMenu14.grid(row = 16, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown14(*args):
    CT = tkvar14.get()
    print(CT)

# link function to change dropdown
tkvar14.trace('w', change_dropdown14)

################################################

# Create a Tkinter variable
tkvar15 = StringVar(window)

popupMenu15 = OptionMenu(window, tkvar15, *YNchoices)
Label(window, text="Paperless Billing", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 17, column = 0, sticky=W)
popupMenu15.config(width= 20, bg="white")
popupMenu15.grid(row = 17, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown15(*args):
    PB = tkvar15.get()
    print(PB)

# link function to change dropdown
tkvar15.trace('w', change_dropdown15)

################################################

# Dictionary with options
Pchoices = {"Electronic check" , "Mailed check", "Bank transfer (automatic)", "Credit card (automatic)" }

# Create a Tkinter variable
tkvar16 = StringVar(window)

popupMenu16 = OptionMenu(window, tkvar16, *Pchoices)
Label(window, text="Payment Method", bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 18, column = 0, sticky=W)
popupMenu16.config(width= 20, bg="white")
popupMenu16.grid(row = 18, column =1, pady=2, sticky=W )

# on change dropdown value
def change_dropdown16(*args):
    PM = tkvar16.get()
    print(PM)

# link function to change dropdown
tkvar16.trace('w', change_dropdown16)

################################################

Label(window, text="Monthly Charges",bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 19, column=0, sticky=W)
text2 = Entry(window, width= 27, bg="white")
text2.grid(row=19, column=1, pady=5, sticky=W)

################################################

Label(window, text="Total Charges",bg = "#0ec7c7",fg="blue",font="none 11 bold").grid(row = 20, column=0, sticky=W)
text3 = Entry(window, width= 27, bg="white")
text3.grid(row=20, column=1, pady=5, sticky=W)

################################################

Button(window, text="SUBMIT", width = 20, command = click ).grid(row = 21, column = 0, pady=5, sticky = W)

################################################

Label(window, text="Predicted Churn",bg = "#0ec7c7", fg="blue", font="none 11 bold").grid(row = 22, column=0, pady=4,sticky=W)

output = Text(window, width= 21, height = 1,  bg="white")
output.grid(row=22 , column=1, sticky=W)



window.mainloop()

"""### ONE EXAMPLE OF INPUTS GIVEN TO GUI AND OUTPUT RECEIVED

![ss1.PNG](attachment:ss1.PNG)

### CONCLUSION

1. We have thus, made a GUI predictor for Company to predict will Churn or not.
2. The model, chosen was a Bagging ensemble model with 88% accuracy.
3. The model was compared to various ensemble techniques and boosting algos.
4. It took a greater amount of time to perform but proved to be much accurate than others.
5. The new boosting algorithms proved to be fast and almost near accurate to the Bagging ensemble.
6. There was a class iimbalance which was fixed with oversampling and thus bagging performed quite good.

### Suggestions on dataset
1. The class imbalance can bias the prediction hence it could be ensured to have almost the same no. of positive an negative churn values data samples.
2. The various values like multiple lines, online backup etc. are not that understandable to the user using the GUI. Hence can be cleared in a better way.
3. No. of data points for class no for churn was of a sufficient amount.
4. No. of features given was also good and contributes in a good way to the class variable churn.
"""

